{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston House Prices\n",
    "\n",
    "The following notebook will attempt to address the following:\n",
    "\n",
    "This assessment concerns the well-known Boston House Prices [1] dataset and the\n",
    "Python [3] packages scipy [2], keras [7], and jupyter [6]. \n",
    "\n",
    "There are three parts which will be addressed:\n",
    "***\n",
    "Describe: Create a git repository and make it available online for the lecturer\n",
    "to clone. The repository should contain all your work for this assessment. Within\n",
    "the repository, create a jupyter [6] notebook that uses descriptive statistics and\n",
    "plots to describe the Boston House Prices [1] dataset. This part is worth 20% of\n",
    "your overall mark.\n",
    "\n",
    "***\n",
    "Infer: To the above jupyter notebook, add a section where you use inferential\n",
    "statistics to analyse whether there is a significant difference in median house prices\n",
    "between houses that are along the Charles river and those that aren’t. You should\n",
    "explain and discuss your findings within the notebook. This part is also worth\n",
    "20%.\n",
    "***\n",
    "Predict: Again using the same notebook, use keras [7] to create a neural network\n",
    "that can predict the median house price based on the other variables in the dataset.\n",
    "You are free to interpret this as you wish — for example, you may use all the other\n",
    "variables, or select a subset. This part is worth 60%.\n",
    "\n",
    "\n",
    "The minimum standard for this assessment is a git repository containing a README file\n",
    "written in Markdown [5] and a jupyter notebook containing your work. The README\n",
    "should contain a summary of your work and provide instructions as to how to run the\n",
    "jupyter notebook and the web application. A better project will be well laid out, clear\n",
    "and concise, and easily understood and run.\n",
    "\n",
    "Note I will rewite the above. I will leave this in the first cell as a guide to myself while completing the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Description of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boston House prices dataset is drawn from the Boston Standard Metropolitan Statisical Area in 1970. Each record describes a Boston suburb or town. There are several attributes included for each of these records. There are 506 records and each of these has 13 variables, which may have an influence on the pricing of the houses in question. The aim is to predict the house prices using the 13 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\n",
      "       'ptratio', 'b', 'lstat', 'medv'],\n",
      "      dtype='object')\n",
      "        crim    zn  indus  chas    nox     rm   age     dis  rad  tax  \\\n",
      "0    0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296   \n",
      "1    0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242   \n",
      "2    0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242   \n",
      "3    0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222   \n",
      "4    0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222   \n",
      "..       ...   ...    ...   ...    ...    ...   ...     ...  ...  ...   \n",
      "501  0.06263   0.0  11.93     0  0.573  6.593  69.1  2.4786    1  273   \n",
      "502  0.04527   0.0  11.93     0  0.573  6.120  76.7  2.2875    1  273   \n",
      "503  0.06076   0.0  11.93     0  0.573  6.976  91.0  2.1675    1  273   \n",
      "504  0.10959   0.0  11.93     0  0.573  6.794  89.3  2.3889    1  273   \n",
      "505  0.04741   0.0  11.93     0  0.573  6.030  80.8  2.5050    1  273   \n",
      "\n",
      "     ptratio       b  lstat  medv  \n",
      "0       15.3  396.90   4.98  24.0  \n",
      "1       17.8  396.90   9.14  21.6  \n",
      "2       17.8  392.83   4.03  34.7  \n",
      "3       18.7  394.63   2.94  33.4  \n",
      "4       18.7  396.90   5.33  36.2  \n",
      "..       ...     ...    ...   ...  \n",
      "501     21.0  391.99   9.67  22.4  \n",
      "502     21.0  396.90   9.08  20.6  \n",
      "503     21.0  396.90   5.64  23.9  \n",
      "504     21.0  393.45   6.48  22.0  \n",
      "505     21.0  396.90   7.88  11.9  \n",
      "\n",
      "[506 rows x 14 columns]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'DESCR'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-5be5c159b9bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mdict_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#this will print out the variables and an explanation of them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboston\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDESCR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;31m#pd.DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5177\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5178\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5179\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'DESCR'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "#Importing the dataset\n",
    "boston = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\")\n",
    "\n",
    "boston\n",
    "\n",
    "#print out the variables within the dataset with their keys and explanations\n",
    "print(boston.keys())\n",
    "print(boston)\n",
    "dict_keys = boston.keys\n",
    "dict_keys()\n",
    "#this will print out the variables and an explanation of them\n",
    "#load the explanations of the keys with sklearn\n",
    "print(boston.DESCR) \n",
    "#pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
      "         0     1     2    3      4      5     6       7    8      9    10  \\\n",
      "0  0.00632  18.0  2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0  15.3   \n",
      "1  0.02731   0.0  7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0  17.8   \n",
      "2  0.02729   0.0  7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0  17.8   \n",
      "3  0.03237   0.0  2.18  0.0  0.458  6.998  45.8  6.0622  3.0  222.0  18.7   \n",
      "4  0.06905   0.0  2.18  0.0  0.458  7.147  54.2  6.0622  3.0  222.0  18.7   \n",
      "\n",
      "       11    12  \n",
      "0  396.90  4.98  \n",
      "1  396.90  9.14  \n",
      "2  392.83  4.03  \n",
      "3  394.63  2.94  \n",
      "4  396.90  5.33  \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "target2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'target2'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e1890c797075>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbopd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mboston\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mboston\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mbopd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'INDUS'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboston\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mbopd2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CRIM'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboston\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: target2"
     ]
    }
   ],
   "source": [
    "# this sets the size of the figure\n",
    "bopd = pd.DataFrame(boston.data)\n",
    "#bopd.columns = boston.feature_names\n",
    "# why is MEDV missing???\n",
    "print(bopd.head())\n",
    "boston.target[:2]\n",
    "boston.target2[:0]\n",
    "bopd['INDUS'] = boston.target\n",
    "bopd2['CRIM'] = boston.target2\n",
    "sns.set(rc = {'figure.figsize':(11.7,8.27)})\n",
    "# this creates a plot of the dataset, based on the MEDV variable, separated into 30 bins to better display the data\n",
    "sns.distplot(bopd['INDUS'], bins = 30)\n",
    "plt.show()\n",
    "# create a correlation matrix between the variables\n",
    "correlation_matrix = bopd.corr().round(2)\n",
    "\n",
    "# create a heatmap of the correlation data, a value close to -1 stands for a negative correlation, a value close to 1 means a positive correlation\n",
    "sns.heatmap(data = correlation_matrix, annot = True)\n",
    "plt.figure(figsize = (20, 5))\n",
    "features = ['LSTAT', 'RM']\n",
    "target = bopd['INDUS']\n",
    "\n",
    "for i, col in enumerate (features):\n",
    "    plt.subplot (l, len(features), i + 1)\n",
    "    x = boston.target2\n",
    "    y = boston.target\n",
    "    plt.scatter(x, y, marker = 'o')\n",
    "    plt.title(col)\n",
    "    plt.xlabel('CRIM')\n",
    "    plt.ylabel('INDUS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boston' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-25ba5d9ea5ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboston\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboston\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'boston' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = bopd[x]\n",
    "y = bopd[y]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 5)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "#training and testing the model\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_train, Y_train)\n",
    "\n",
    "#evaluating the training set\n",
    "y_train_predict = lin_model.predict(X_train)\n",
    "rmse = (up.sqrt(mean_squared.error(Y_train, y_train_predict)))\n",
    "r2 = r2_score(Y_train, y_train_predict)\n",
    "print(\"The performance of the model for the training set is\")\n",
    "print(\"**************************************\")\n",
    "print('The RMSE is {}'.format(rmse))\n",
    "print('The R2 score is{}'.format(r2))\n",
    "print(\"\\n\")\n",
    "\n",
    "#evaluating the test set\n",
    "y_test_predict = lin_model.predict(X_test)\n",
    "rmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n",
    "r2 = r2_score(Y_test, _test_predict)\n",
    "print(\"The performance of the model for the testing set is\")\n",
    "print(\"**************************************\")\n",
    "print('The RMSE is {}'.format(rmse))\n",
    "print('The R2 score is {}'.format(r2))\n",
    "\n",
    "# TODO: Import 'train_test_split'\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# TODO: Shuffle and split the data into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, prices, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inferential statistics for the Charles River House Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still need to get the Charles River house prices data separated (CHAS >> dummy variable for the Charles River 1 for yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boston' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b7813b9c2d03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mboston\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcharlesRiver\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CHAS'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboston\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Minimum price of the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mminimum_price\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'boston' is not defined"
     ]
    }
   ],
   "source": [
    "boston.target3[:3]\n",
    "charlesRiver['CHAS'] = boston.target3\n",
    "# Minimum price of the data\n",
    "minimum_price = np.amin(prices)\n",
    "\n",
    "# Maximum price of the data\n",
    "maximum_price = np.amax(prices)\n",
    "\n",
    "# Mean price of the data\n",
    "mean_price = np.mean(prices)\n",
    "\n",
    "# Median price of the data\n",
    "median_price = np.median(prices)\n",
    "\n",
    "# Standard deviation of prices of the data\n",
    "std_price = np.std(prices)\n",
    "\n",
    "# Show the calculated statistics\n",
    "print(\"Statistics for Boston housing dataset:\\n\")\n",
    "print(\"Minimum price: ${}\".format(minimum_price)) \n",
    "print(\"Maximum price: ${}\".format(maximum_price))\n",
    "print(\"Mean price: ${}\".format(mean_price))\n",
    "print(\"Median price ${}\".format(median_price))\n",
    "print(\"Standard deviation of prices: ${}\".format(std_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "discuss the findings briefly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Predict: Creating a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.python.keras.datasets.boston_housing.load_data\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "\n",
    "# take a look at the data\n",
    "\n",
    "print(f'Training data : {train_data.shape}')\n",
    "print(f'Test data : {test_data.shape}')\n",
    "print(f'Training sample : {train_data[0]}')\n",
    "print(f'Training target sample : {train_targets[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='mse',\n",
    "              metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "    print(f'Processing fold # {i}')\n",
    "    val_data = train_data[i * num_val_samples: (i+1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i+1) * num_val_samples]\n",
    "    \n",
    "    partial_train_data = np.concatenate(\n",
    "                            [train_data[:i * num_val_samples],\n",
    "                            train_data[(i+1) * num_val_samples:]],\n",
    "                            axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "                            [train_targets[:i * num_val_samples],\n",
    "                            train_targets[(i+1)*num_val_samples:]],\n",
    "                            axis=0)\n",
    "    model = build_model()\n",
    "    model.fit(partial_train_data,\n",
    "              partial_train_targets,\n",
    "              epochs=num_epochs,\n",
    "              batch_size=1,\n",
    "              verbose=0)\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'all_scores : {all_scores}')\n",
    "print(f'mean all scores : {np.mean(all_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.fit(train_data, train_targets, epochs=80, batch_size=16, verbose=0)\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)\n",
    "test_mae_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "discuss the findings briefly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "https://www.ritchieng.com/machine-learning-project-boston-home-prices/\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html\n",
    "https://towardsdatascience.com/linear-regression-on-boston-housing-dataset-f409b7e4a155\n",
    "https://towardsdatascience.com/machine-learning-project-predicting-boston-house-prices-with-regression-b4e47493633d\n",
    "https://www.kaggle.com/shanekonaung/boston-housing-price-dataset-with-keras\n",
    "https://www.kaggle.com/callmejeffery/boston-house-price-with-keras\n",
    "https://hackernoon.com/build-your-first-neural-network-to-predict-house-prices-with-keras-3fb0839680f4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
